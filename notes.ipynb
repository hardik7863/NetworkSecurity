{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this project we will not use template to create the project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly created the environ python 3.10\n",
    "\n",
    "then requirements.txt\n",
    "\n",
    "then .github/workflows/main.yml\n",
    " \n",
    "then Network_Data :- this will consisist of dataset\n",
    "\n",
    "then notebooks\n",
    " \n",
    "then network security :- complete structure of the code will be inside this we will create a file __init__.py this will consider folder as package helps in importing in the other files\n",
    "then we will create component ,looging,entity,execption,utils,constant,pipeline,cloud folders\n",
    "\n",
    "then setup.py file in this file we usually note the generic information about the author,version of the project\n",
    "\n",
    "then Dockerfile \n",
    "\n",
    "add basic library to the requirements.txt file\n",
    "\n",
    "then create the.env file\n",
    " \n",
    "now connect to git repository \n",
    "\n",
    "then create __init__.py in seach folder of networksecurity\n",
    "\n",
    "since github do not add the empty folder and we net __init__.py file also for treating folders as packages\n",
    "\n",
    "\n",
    "In company we follows this kind of structure\n",
    "\n",
    "now lets add some code to setup.py\n",
    "\n",
    "\n",
    "add pymongo and certifi to the requirements.txt\n",
    "\n",
    "add this -e . to requirements.txt at the end of the project\n",
    "this means it will install the requirements from setup folder and it will create the a folder having complete information about the author author email,packages details.etc\n",
    "\n",
    "here -e . is directing towards setup.py file\n",
    "\n",
    "then use the command in the terminal pip install -r requiremnts.txt \n",
    "\n",
    "this will create a folder which has some information which is define in setup.py file\n",
    "This is done at the end when your project is complete for timebeing comment out the -e . in requirements.txt. this is done when you are involving PyPI package or deploying on cloud \n",
    "\n",
    "after commenting delete the networkSecurity.egg folder created due to setup file \n",
    "\n",
    "now create a file logger.py in logging folder and add logging code\n",
    "\n",
    "then now create execption.py file inside exception folder\n",
    "\n",
    "now checking the exception.py working well or not by run the command \n",
    " python networksecurity/exception/exception.py\n",
    "\n",
    "refer Etlpipeline topic in finalnetworkingsecurity.pdf\n",
    "\n",
    "our main object of the project is to predict wheather the website is fake/malcious website or legit website\n",
    "\n",
    "inorder to perform ETL we will be requiring mangodb database we will be using mongo db atlas \n",
    "\n",
    "create an free account and then fill some useless details then create cluster and choose free M0 which is free and deply the cluster and create your database username setup the connection and then choose drivers and the pymongo[srv] to the requirements.txt and create the done on mangodb wait for cluster to get created \n",
    "\n",
    "click on connect choose drivers and view full code sample to check the connection working or not\n",
    "\n",
    "create a file push_data.py and add the sample code in that file\n",
    "\n",
    "first use this command to check the mongo db connection:\n",
    " \n",
    " pip install -r requirements.txt\n",
    "\n",
    " python push_data.py\n",
    " o/p:-Pinged your deployment. You successfully connected to MongoDB!\n",
    "\n",
    "now save your mongodb_url to .env so that no one can see the url\n",
    "\n",
    "and copy the content of push_data.py to new file test_mongodb.py\n",
    "and remove the content from push_data.py \n",
    "\n",
    "we are using certifi library is used to provide trusted certificates which makes the request  legit and trusted and mongodb will not consider it as spam or some malicious activity\n",
    "\n",
    "ADD SOME CODE TO THE push_data.py then we will extract the data from the location and we need to convert the csv file into json format since we are using mongodb \n",
    "\n",
    "then run python push_data.py and see the changes in the cluster then collection in mongodb atlas\n",
    "\n",
    "now look to the data ingestion in finalnetworksecurity.pdf\n",
    "\n",
    "now create a data_ingestion.py file in /networksecurity/components/\n",
    "\n",
    "\n",
    "then create config_entity.py inside the /networksecurity/entity/\n",
    "\n",
    "then create training_pipeline folder with __init__.py  inside the /networksecurity/constant/\n",
    "\n",
    "then add code to the config_entity.py \n",
    "\n",
    "then add code to the data_ingestion.py in component folder of networksecurity\n",
    "\n",
    "add scikit-learn to requirements.txt\n",
    "\n",
    "in data_ingestion.py we are importing the data from mongodb \n",
    "\n",
    "we will create the artifact_entity.py  in entity folder of network security\n",
    "\n",
    "in artifact_entity.py we will save the file train.csv and test.csv\n",
    "\n",
    "then create main.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note**:\n",
    "it is better to read the data from mongodb and store locally instaed of reading from mongodb again and again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we read the data then data should follow the schema if there is 10 features if given  input is in different datatype then it should show error because it will be affecting the model and this change in data is called data drift.we cannot use these changed data to train our model .we need to create the data drift report \n",
    "\n",
    "- update trainingpipeline/__init__.py\n",
    "\n",
    "- update entity/entity_config.py\n",
    "\n",
    "- update entity/artifact_entity.py\n",
    "\n",
    "- Create the components/data_validation.py\n",
    "\n",
    "- create data_schema/schema.yaml\n",
    "\n",
    "- create  main_utils/utils.py and main_utils/__init__.py\n",
    "\n",
    "- pyaml to requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update trainingpipeline/__init__.py\n",
    "\n",
    "- update entity/entity_config.py\n",
    "\n",
    "- update entity/artifact_entity.py\n",
    "\n",
    "- Create the components/data_transformation.py\n",
    "\n",
    "- we have note used smote technique as mention in the notes because the dataset is balanced\n",
    "no need of this feature engineering technique\n",
    "\n",
    "- updated the utils/main_utils/utils.py with array functionalities\n",
    "\n",
    "- KNN Imputer is applied after removing the target column basically what knn imputer do is \n",
    "it replaces the missing values with the mean of the nearest neighbours we have here used 3 neighbours (feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update trainingpipeline/__init__.py\n",
    "\n",
    "- update entity/entity_config.py\n",
    "\n",
    "- update entity/artifact_entity.py\n",
    "\n",
    "- Create the components/model_trainer.py\n",
    "\n",
    "- create the utils/ml_utils/metric/classification_metrics.py and utils/ml_utils/model/estimator.py\n",
    "\n",
    "- instead of creating the model evaluation module we have added modl evaluationin the utils/main_utils/utils.py\n",
    "\n",
    "- best algo  is return to the model_trainer.py \n",
    "\n",
    "-and for prediction predict function is define in utils/ml_utils/model/estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Tracking**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update dagshub ,mlflow in requirements.txt\n",
    "\n",
    "- connect to the dagshub \n",
    "\n",
    "- set up the pipeline/batch_prediction.py and training_pipeline.py\n",
    "\n",
    "- create the app.py\n",
    "\n",
    "- created the template folder  and valid_data folder for batch prediction\n",
    "\n",
    "\n",
    "- we should use s3 bucket for pkl file in case when it is large in size because in our case the pickle file is small we have used to github "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
